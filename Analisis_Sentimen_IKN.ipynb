{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PENGUMPULAN DATA"
      ],
      "metadata": {
        "id": "as5wA3xqSeDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PySastrawi"
      ],
      "metadata": {
        "id": "Ny7C6UaPSeqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import model_selection\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import svm\n",
        "\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.util import ngrams\n",
        "\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
      ],
      "metadata": {
        "id": "fE_QBvZdSlvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Koneksi ke Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rfPkjGAbSlo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read the dataset\n",
        "df = pd.read_excel('/content/drive/MyDrive/Machine Learning/Analisis Sentimen IKN/Dataset/Sentimen Pengguna Twitter Pada Topik IKN.xlsx')\n",
        "df['Tweet']"
      ],
      "metadata": {
        "id": "BQQ_BkkOSlVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KSOuHtS_TNAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "pHqKVA2STO86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualisasi Data\n",
        "plt.hist(df.Sentimen)"
      ],
      "metadata": {
        "id": "wRzguNNuTPcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRE-PROCESSING TEXT"
      ],
      "metadata": {
        "id": "PH9CzFJwTjhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cleansing Data\n",
        "def cleansing(text):\n",
        "    #Menghilangkan tanda kurung\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    #Menghilangkan Tanda Baca\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    #Menghilangkan Special Karakter\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    #Menghilangkan Single Character\n",
        "    text = re.sub('\\s+[a-zA-Z]\\s+', '', text)\n",
        "    #Menghilangkan Link\n",
        "    text = re.sub(r\"http\\S+\", \"\", text, flags=re.MULTILINE)\n",
        "    #Menghilangkan Hashtag\n",
        "    text = re.sub(r'\\B#\\S+','', text)\n",
        "    #Mengganti Multiple Spaces ke Single Spaces\n",
        "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
        "    #Menghilangkan Spaces di awal\n",
        "    text = text.strip()\n",
        "    return text\n",
        "df['CLEANSING']= df['Tweet'].apply(lambda x: cleansing(x))"
      ],
      "metadata": {
        "id": "4Uvjf0rnTkOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "63cVsq0JT7l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel(\"/content/drive/MyDrive/Machine Learning/Analisis Sentimen IKN/Hasil QE/Cleansing.xlsx\")"
      ],
      "metadata": {
        "id": "vOt02jIkT7ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CaseFolding\n",
        "def case_folding(text):\n",
        "    text = text.lower()\n",
        "    return text\n",
        "df['CASEFOLDING']= df['CLEANSING'].apply(lambda x: case_folding(x))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "W-UtYZb_T7dS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "zuC755fST7Zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization\n",
        "def tokenization(text):\n",
        "    text = re.split('\\W+', text)\n",
        "    return text\n",
        "df['TOKENIZATION']= df['CASEFOLDING'].apply(lambda x: tokenization(x))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "IGqIT3qkT7Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read txt stopword using pandas\n",
        "txt_stopword = pd.read_excel(\"/content/drive/MyDrive/Machine Learning/Analisis Sentimen IKN/Kamus/kamus_stopword.xlsx\", names= [\"stopwords\"], header = None)\n",
        "\n",
        "# convert stopword string to list & append additional stopword\n",
        "list_stopwords = txt_stopword['stopwords'].values.tolist()\n",
        "\n",
        "# ---------------------------------------------------------------------------------------\n",
        "\n",
        "# convert list to dictionary\n",
        "list_stopwords = set(list_stopwords)\n",
        "\n",
        "#remove stopword pada list token\n",
        "def stopwords_removal(words):\n",
        "    return [word for word in words if word not in list_stopwords]\n",
        "\n",
        "df['STOP REMOVAL'] = df['TOKENIZATION'].apply(stopwords_removal)\n",
        "\n",
        "print(df['STOP REMOVAL'].head())"
      ],
      "metadata": {
        "id": "3db3YKyTT7Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel(\"/content/drive/MyDrive/Machine Learning/Analisis Sentimen IKN/Hasil QE/StopwordRemoval.xlsx\")"
      ],
      "metadata": {
        "id": "cX3MtT4gT7MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#StopwordRemoval\n",
        "final = pd.DataFrame(df, columns= ['Sentimen', 'STOP REMOVAL'])\n",
        "final"
      ],
      "metadata": {
        "id": "uIjLIJG2T7Ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install swifter"
      ],
      "metadata": {
        "id": "OwJfzloyVcAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming\n",
        "import swifter\n",
        "# create stemmer\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "# stemmed\n",
        "def stemmed_wrapper(term):\n",
        "    return stemmer.stem(term)\n",
        "\n",
        "term_dict = {}\n",
        "\n",
        "for document in final['STOP REMOVAL']:\n",
        "    for term in document:\n",
        "        if term not in term_dict:\n",
        "            term_dict[term] = ' '\n",
        "\n",
        "print(len(term_dict))\n",
        "print(\"------------------------\")\n",
        "\n",
        "for term in term_dict:\n",
        "    term_dict[term] = stemmed_wrapper(term)\n",
        "    print(term,\":\" ,term_dict[term])\n",
        "\n",
        "print(term_dict)\n",
        "print(\"------------------------\")\n",
        "\n",
        "# apply stemmed term to dataframe\n",
        "def get_stemmed_term(document):\n",
        "    return [term_dict[term] for term in document]\n",
        "\n",
        "final['STEMMING'] = final['STOP REMOVAL'].apply(get_stemmed_term)"
      ],
      "metadata": {
        "id": "3GvRs-LGVb7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final.head()"
      ],
      "metadata": {
        "id": "GfzTU_5vVb4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kbba = pd.read_excel('/content/drive/MyDrive/Machine Learning/Analisis Sentimen IKN/Kamus/kamus_normalisasi.xlsx')"
      ],
      "metadata": {
        "id": "gXaozfyqVbzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kbba"
      ],
      "metadata": {
        "id": "D7yLhtsNVbq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalisasi\n",
        "\n",
        "normalizad_word_dict = {}\n",
        "\n",
        "for index, row in kbba.iterrows():\n",
        "    if row[0] not in normalizad_word_dict:\n",
        "        normalizad_word_dict[row[0]] = row[1]\n",
        "\n",
        "def normalized_term(document):\n",
        "    return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]\n",
        "\n",
        "final['NORMALISASI'] = final['STEMMING'].apply(normalized_term)\n",
        "\n",
        "final['NORMALISASI'].head(10)"
      ],
      "metadata": {
        "id": "Fr4O2_ECWKRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_normalisasi(text):\n",
        "    text = np.array(text)\n",
        "    text = ' '.join(text)\n",
        "\n",
        "    return text\n",
        "final['NORMALISASI'] = final['NORMALISASI'].apply(lambda x: fit_normalisasi(x))\n",
        "final.head()"
      ],
      "metadata": {
        "id": "KN5gzC9-WKLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final.to_excel(\"/content/drive/MyDrive/Machine Learning/Analisis Sentimen IKN/Hasil QE/Normalisasi.xlsx\")"
      ],
      "metadata": {
        "id": "YvyuY-uHWJ-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NORMALISASI NEGASI\n",
        "def convert_negasi(text):\n",
        "    text = re.sub(\"tidak \", 'tidak', text, flags=re.MULTILINE)\n",
        "    text = re.sub(\"jangan \", 'jangan', text, flags=re.MULTILINE)\n",
        "    text = re.sub(\"belum \", 'belum', text, flags=re.MULTILINE)\n",
        "    text = re.sub(\"bukan \", 'bukan', text, flags=re.MULTILINE)\n",
        "    text = re.sub(\"tanpa \", 'tanpa', text, flags=re.MULTILINE)\n",
        "    text = re.sub(\"bukanlah \", 'bukanlah', text, flags=re.MULTILINE)\n",
        "    text = re.sub(\"tak \", 'tak', text, flags=re.MULTILINE)\n",
        "    text = re.sub(\"anti \", 'anti', text, flags=re.MULTILINE)\n",
        "    return text\n",
        "final['NORMALISASI']= final['NORMALISASI'].apply(lambda x: convert_negasi(x))\n",
        "final"
      ],
      "metadata": {
        "id": "TqKTy1B1WaMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "negative_words =' '.join([text for text in final['NORMALISASI'][final['Sentimen'] == 'Negatif']])\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=500, random_state = 0, max_font_size = 110).generate(negative_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title('The Negative Words')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sb_fCV3NWaDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "positif_words =' '.join([text for text in final['NORMALISASI'][final['Sentimen'] == 'Positif']])\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=500, random_state = 0, max_font_size = 110).generate(positif_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title('The Positif Words')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VUP--OLNWl2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Merubah Sentimen ke Polarity\n",
        "#Konversi Sentimen Ke Polaritas\n",
        "def convert(polarity):\n",
        "    if polarity == 'Positif':\n",
        "        return 1\n",
        "    else:\n",
        "        return -1"
      ],
      "metadata": {
        "id": "U-X_3-x7WpMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final['Polarity'] = final['Sentimen'].apply(convert)\n",
        "final"
      ],
      "metadata": {
        "id": "SPMO0zRCWpEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final.to_excel(\"/content/drive/MyDrive/Machine Learning/Analisis Sentimen IKN/Hasil QE/Setelah Pre-Processing.xlsx\")"
      ],
      "metadata": {
        "id": "HC692-JfWvWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split Data\n",
        "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(final['NORMALISASI'],final['Polarity'], test_size=0.1, random_state=30)\n",
        "#print(X_Test)"
      ],
      "metadata": {
        "id": "8Jd7cXjPW0rC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Encoder = LabelEncoder()\n",
        "Train_Y = Encoder.fit_transform(Train_Y)\n",
        "Test_Y = Encoder.fit_transform(Test_Y)"
      ],
      "metadata": {
        "id": "eP-6-XhuW4Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PEMBOBOTAN TF-IDF"
      ],
      "metadata": {
        "id": "Oen5RVTTW7JE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Tfidf_vect = TfidfVectorizer()\n",
        "Tfidf_vect.fit(final['NORMALISASI'])\n",
        "\n",
        "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
        "Test_X_Tfidf = Tfidf_vect.transform(Test_X)"
      ],
      "metadata": {
        "id": "QNgAzNDVW-bT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Train_X_Tfidf.shape"
      ],
      "metadata": {
        "id": "plX_IcpcXEGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Train_Y.shape"
      ],
      "metadata": {
        "id": "OHu39i3PXEDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Train_X_Tfidf)"
      ],
      "metadata": {
        "id": "edX09C2jXD_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Test_X_Tfidf)"
      ],
      "metadata": {
        "id": "QfKXVS2aXICV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUERY EXPANSION"
      ],
      "metadata": {
        "id": "cRtqB7ouXPcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import expansion\n",
        "import xlrd\n",
        "\n",
        "queryList = []\n",
        "# we want to keep the list of unique queries\n",
        "uniqueQuery = []\n",
        "\n",
        "workbook = xlrd.open_workbook('data/m.xls')\n",
        "sheet_names = workbook.sheet_names()\n",
        "xl_sheet = workbook.sheet_by_name(sheet_names[0])\n",
        "\n",
        "for row_idx in range(0, xl_sheet.nrows):    # Iterate through rows\n",
        "        cell_id = xl_sheet.cell(row_idx, 0).value  # Get id cell\n",
        "        cell_topic = xl_sheet.cell(row_idx, 2).value  # Get topic cell\n",
        "        if str(cell_topic) not in uniqueQuery:\n",
        "            queryList.append(str(cell_id) + \",\" + str(cell_topic))\n",
        "            uniqueQuery.append(str(cell_topic))\n",
        "\n",
        "# the list of queries that we want to expand\n",
        "expansion.run(queryList)"
      ],
      "metadata": {
        "id": "fDVvKZ9OXPxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# METODE KLASIFIKASI SVM"
      ],
      "metadata": {
        "id": "-txHOICaXjJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
        "svm = SVC()\n",
        "svm_tuned = GridSearchCV(svm, hyperparameters)"
      ],
      "metadata": {
        "id": "XIkER3PzXjlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_tuned.fit(Train_X_Tfidf, Train_Y)"
      ],
      "metadata": {
        "id": "wAUNJOAKXsRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm = SVC(C=1, kernel='linear', degree=3, gamma='scale')\n",
        "svm.fit(Train_X_Tfidf,Train_Y)"
      ],
      "metadata": {
        "id": "MAyoQVZEXv58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'model_svm_IKN.pkl'\n",
        "pickle.dump(svm, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "3zmf5bvyX2xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "result = loaded_model.score(Test_X_Tfidf, Test_Y)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "pI0s0XpFX6Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EVALUASI KERJA"
      ],
      "metadata": {
        "id": "8gfjZTOzX-vY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HASIL AKURASI, RECALL, PRECISSION, F1 SCORE\n",
        "\n",
        "predict_test = svm.predict(Test_X_Tfidf)\n",
        "\n",
        "print(\"SVM Accuracy Score = \", accuracy_score(predict_test, Test_Y)*100)\n",
        "print(\"SVM Precision Score = \", recall_score(predict_test, Test_Y)*100)\n",
        "print(\"SVM Recall Score = \", precision_score(predict_test, Test_Y)*100)\n",
        "print(\"SVM f1 Score = \", f1_score(predict_test, Test_Y)*100)"
      ],
      "metadata": {
        "id": "rUP-HmToX_Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report = classification_report(Test_Y, predict_test)\n",
        "print(report) # print classification_report"
      ],
      "metadata": {
        "id": "nbgYk8nwYJRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                         normalize=False,\n",
        "                         title=None,\n",
        "                         cmap=plt.cm.Blues):\n",
        "\n",
        "    \"\"\"\n",
        "    This functions\n",
        "    normalize=True\n",
        "    \"\"\"\n",
        "\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    classes = classes[unique_labels(y_true, y_pred)]\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "          yticks=np.arange(cm.shape[0]),\n",
        "          xticklabels=classes, yticklabels=classes,\n",
        "          title=title,\n",
        "          ylabel='True label',\n",
        "          xlabel='Predicted label')\n",
        "\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "            rotation_mode=\"anchor\")\n",
        "\n",
        "    fmt= '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                   ha=\"center\", va=\"center\",\n",
        "                   color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "            fig.tight_layout()\n",
        "            return ax\n",
        "\n",
        "    np.set_printoptions(precision=2)"
      ],
      "metadata": {
        "id": "V9vK3yxIYJLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = Test_Y\n",
        "plot_confusion_matrix(Test_Y, predict_test, classes=class_names,\n",
        "                     title='Confusion matrix, without normalization')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "de1yE6uSYI5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('CONFUSION MATRIX')\n",
        "print('[TP  FN]')\n",
        "print('[FP  TN]')\n",
        "confusion_matrix(Test_Y, predict_test, labels=[1, 0])"
      ],
      "metadata": {
        "id": "pF4VNriYYcpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HASIL AKURASI, RECALL, PRECISSION, F1 SCORE DENGAN CONFUSION MATRIX\n",
        "\n",
        "TP = 87\n",
        "FN = 6\n",
        "FP = 0\n",
        "TN = 107\n",
        "\n",
        "accuracy = (TP+TN)/(TP+FN+FP+TN)\n",
        "precision = TP/(TP+FP)\n",
        "recall = TP/(TP+FN)\n",
        "f1 = 2 *precision*recall/(precision+recall)\n",
        "\n",
        "print(\"HASIL AKURASI, RECALL, PRECISION, F1 SCORE DENGAN CONFUSION MATRIX \\n\")\n",
        "\n",
        "print(\"SVM Accuracy Score = \", accuracy*100 ,\"%\")\n",
        "print(\"SVM Precision Score = \", precision*100,\"%\")\n",
        "print(\"SVM Recall Score = \", recall*100,\"%\")\n",
        "print(\"SVM f1 Score = \", f1*100,\"%\")"
      ],
      "metadata": {
        "id": "ZqemDgbVYtsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EVALUASI VALIDASI K-FOLD CROSS VALIDATION"
      ],
      "metadata": {
        "id": "ukq4uFoiY8SC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import cross_val_score\n",
        "#10 FOLD\n",
        "scores_test = cross_val_score(svm, Test_X_Tfidf, Test_Y, cv = 10)\n",
        "print(\"Hasil Akurasi menggunakan 10 Fold Cross Validation \\n\")\n",
        "for i in range(10):\n",
        "    print(\"Akurasi dari SVM Iterasi ke -\", i+1, \"  : {0:2}\".format(scores_test[i,]*100))\n",
        "print(\"\\n\",\"Rata-Rata Akurasi dari SVM menggunakan Cross Validation :\", scores_test.mean()*100)"
      ],
      "metadata": {
        "id": "mWahFHcpZFui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10 FOLD - Precission\n",
        "\n",
        "scores_test = cross_val_score(svm_tuned, Test_X_Tfidf, Test_Y, cv = 10, scoring='precision')\n",
        "scores_test\n",
        "print(\"Hasil Presisi menggunakan 10 Fold Cross Validation \\n\")\n",
        "for i in range(10):\n",
        "    print(\"Presisi dari SVM Iterasi ke -\", i+1, \"  : {0:2}\".format(scores_test[i,]*100))\n",
        "print(\"\\n\",\"Rata-Rata Presisi dari SVM menggunakan Cross Validation :\", scores_test.mean()*100)"
      ],
      "metadata": {
        "id": "qyUMkCpJZIej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10 FOLD - Recall\n",
        "\n",
        "scores_test = cross_val_score(svm_tuned, Test_X_Tfidf, Test_Y, cv = 10, scoring='recall')\n",
        "scores_test\n",
        "print(\"Hasil Recall menggunakan 10 Fold Cross Validation \\n\")\n",
        "for i in range(10):\n",
        "    print(\"Recall dari SVM Iterasi ke -\", i+1, \"  : {0:2}\".format(scores_test[i,]*100))\n",
        "print(\"\\n\",\"Rata-Rata Recall dari SVM menggunakan Cross Validation :\", scores_test.mean()*100)"
      ],
      "metadata": {
        "id": "xTbRoTmmZJZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10 FOLD - F1 SCORE\n",
        "\n",
        "scores_test = cross_val_score(svm_tuned, Test_X_Tfidf, Test_Y, cv = 10, scoring='f1')\n",
        "scores_test\n",
        "print(\"Hasil F1 Score menggunakan 10 Fold Cross Validation \\n\")\n",
        "for i in range(10):\n",
        "    print(\"F1 Score dari SVM Iterasi ke -\", i+1, \"  : {0:2}\".format(scores_test[i,]*100))\n",
        "print(\"\\n\",\"Rata-Rata F1 Score dari SVM menggunakan Cross Validation :\", scores_test.mean()*100)"
      ],
      "metadata": {
        "id": "9YsX5h39ZJXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify(tweet):\n",
        "    pred = svm.predict(Tfidf_vect.transform([tweet]))\n",
        "    if pred == 1:\n",
        "        return \"Positif\"\n",
        "    return \"Negatif\""
      ],
      "metadata": {
        "id": "13gZ_3cnZJU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify('IKN akan jadi representasi bangsa yang unggul')"
      ],
      "metadata": {
        "id": "c2n9csU2ZJSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify('IKN memberikan dampak positif')"
      ],
      "metadata": {
        "id": "HcA7efp3ZJQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify('proyek bengkak')"
      ],
      "metadata": {
        "id": "Z_EUHiVJZJNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify('IKN proyek gagal')"
      ],
      "metadata": {
        "id": "z5QrlpKbZz6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify('mari kita dukung pembangunan IKN')"
      ],
      "metadata": {
        "id": "iN7xdvMoZz37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify('IKN bangkitkan ekonomi Indonesia')"
      ],
      "metadata": {
        "id": "Q7E2GKfVZz0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify('IKN kota dunia untuk semua')"
      ],
      "metadata": {
        "id": "KRdkD-3eZzxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify('menuju Indonesia maju dan berkembang')"
      ],
      "metadata": {
        "id": "OEJC-v0bZzuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify('pusat ekonomi masa depan')"
      ],
      "metadata": {
        "id": "x4qHwfiTZzqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify('forest city')"
      ],
      "metadata": {
        "id": "b58O7Ly-ZzfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify('konsep Green City')"
      ],
      "metadata": {
        "id": "cRqX74qCZJLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify('investor ragu tanam modal')"
      ],
      "metadata": {
        "id": "nMuIkVjEZJIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify('IKN lebih penting dari entaskan kemiskinan')"
      ],
      "metadata": {
        "id": "sgGNmDhnaeGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify('curiga IKN')"
      ],
      "metadata": {
        "id": "WBcBtpEKad_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#STEMMING\n",
        "# create stemmer\n",
        "factory = StemmerFactory()\n",
        "stemming = factory.create_stemmer()\n",
        "# Using a Python list comprehension method to apply to all words in my_list\n",
        "def stem(text):\n",
        "    my_list = text\n",
        "    stemmed_list = [stemming.stem(word) for word in my_list]\n",
        "    return (stemmed_list)"
      ],
      "metadata": {
        "id": "L15oE5CcagZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    text = cleansing(text)\n",
        "    text = case_folding(text)\n",
        "    text = tokenization(text)\n",
        "    text = stopwords_removal(text)\n",
        "    text = stem(text)\n",
        "    text = normalized_term(text)\n",
        "    text = fit_normalisasi(text)\n",
        "    text = convert_negasi(text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "ajCGuJspaj-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a= preprocess('Dukung pembangunan IKN di Kalimantan Timur https/twitter.com #dukungIKN')\n",
        "a"
      ],
      "metadata": {
        "id": "t7knpfSdaj2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify(a)"
      ],
      "metadata": {
        "id": "-sKuIOACasxN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}